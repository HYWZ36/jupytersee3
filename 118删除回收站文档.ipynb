{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     103,
     119,
     128,
     175,
     179
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:245: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:263: DeprecationWarning: use driver.switch_to.default_content instead\n",
      "c:\\python27\\lib\\site-packages\\ipykernel_launcher.py:277: DeprecationWarning: use driver.switch_to.default_content instead\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[@id='batch']/div/div/label\"}\n  (Session info: chrome=83.0.4103.116)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3f40cc3debd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWenkuspider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mThreadpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 单线程跑的慢，但是基本很稳定没有被cnvd封杀。本次跑完整个cnvd库大概十天左右，期间网络波动中断需要维护代码，及爬虫重新爬的页面范围。\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m     \u001b[1;31m# pool.map(a.run(),self.parse())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-3f40cc3debd5>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//div[@id='batch']/div/div/label\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_link_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"批量彻底删除\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[1;34m(self, xpath)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//div/td[1]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0;32m    977\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m             'value': value})['value']\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.pyc\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//div[@id='batch']/div/div/label\"}\n  (Session info: chrome=83.0.4103.116)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#CNVD爬虫\n",
    "import requests\n",
    "from lxml import etree\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import codecs\n",
    "from datetime import date\n",
    "from multiprocessing.dummy import Pool as Threadpool\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import alert_is_present\n",
    "from selenium.webdriver.common.alert import Alert\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import cookielib\n",
    "import urllib2\n",
    "\n",
    "# 判断是否爬取页面信息，为1则重新爬并缓存页面，为0则直接从缓存页面捕获信息。\n",
    "is_crwal = 0\n",
    "\n",
    "class Wenkuspider(object):\n",
    "    def __init__(self):\n",
    "        self.__file__ = r'C:/Users/wz/Desktop/lvmeng/paloalt_submit00.json'\n",
    "        self.__link__ = r\"https://security.paloaltonetworks.com\"\n",
    "        self.__loadeddir__ = 'loadedhtml0'\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\"}\n",
    "        # 如果从某处断线了，可以更改起始的url地址\n",
    "        #self.start_url = \"http://www.cnvd.org.cn/flaw/list.htm\"\n",
    "        self.count = 0\n",
    "        self.cookies = self.get_cookies()\n",
    "\n",
    "    # 保存网页源码到本地，local_filename表示当前页面的完整名称\n",
    "    def download_document(self, url, local_filename, retry=3):\n",
    "#         print(url)\n",
    "#         print(local_filename)\n",
    "        local_dirname = os.path.dirname(local_filename)\n",
    "        if not os.path.isdir(local_dirname):\n",
    "            print \"create dir:\", local_dirname\n",
    "            os.makedirs(local_dirname)\n",
    "        req = urllib2.Request(url)\n",
    "        req.add_header('User-Agent',\n",
    "                       'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3135.5 Safari/537.36')\n",
    "        retry_init = retry\n",
    "        while retry:\n",
    "            try:\n",
    "                resp = urllib2.urlopen(req, timeout=30)\n",
    "                content = resp.read()\n",
    "                f = open(local_filename, \"w\")\n",
    "                f.write(content)\n",
    "                f.close()\n",
    "                break\n",
    "            except Exception, e:\n",
    "                print e\n",
    "                retry = retry - 1\n",
    "                print \"retry %s after %ss ...\" % (url, retry_init - retry)\n",
    "                sleep(retry_init - retry)\n",
    "    \n",
    "    # 获取网站cookie\n",
    "    def get_cookies(self):\n",
    "        # 构建一个CookieJar对象实例来保存cookie\n",
    "        cookiejar = cookielib.CookieJar()\n",
    "        # 使用HTTPCookieProcessor()来创建cookie处理器对象，参数为CookieJar()对象\n",
    "        handler=urllib2.HTTPCookieProcessor(cookiejar)\n",
    "        # 通过 build_opener() 来构建opener\n",
    "        opener = urllib2.build_opener(handler)\n",
    "        # 4. 以get方法访问页面，访问之后会自动保存cookie到cookiejar中\n",
    "        opener.open(self.__link__)\n",
    "        # 可以按标准格式将保存的Cookie打印出来\n",
    "        cookieStr = ''\n",
    "        for item in cookiejar:\n",
    "            cookieStr += \"'\"+item.name + \"':'\" + item.value + \"',\"\n",
    "        cookie = ast.literal_eval('{'+cookieStr+'}')\n",
    "#         print(cookie)\n",
    "        return cookie\n",
    "    # get请求\n",
    "    def parse(self, i):\n",
    "#         time.sleep(random.randint(5, 10))\n",
    "        self.count += 1\n",
    "        print('每两个请求重新获取cookie，重置完成后，现在是第%s个'%str(self.count))\n",
    "        if(self.count == 2):\n",
    "            self.cookies = self.get_cookies()\n",
    "            self.count = 0\n",
    "        url=self.__link__+'/?page=%s&sort=date'%str(i)\n",
    "        print(1)\n",
    "        print(url)\n",
    "        html = requests.get(url, headers=self.headers,\n",
    "                            cookies=self.cookies).content\n",
    "#         print(html)\n",
    "        html = etree.HTML(html)\n",
    "#         print('第%s页'%str(int(i/20+1)))\n",
    "#         print(html)\n",
    "        return html\n",
    "\n",
    "    # get请求网页具体信息\n",
    "    def parsedetail(self, url):\n",
    "#         time.sleep(random.randint(5, 10))\n",
    "        self.count += 1\n",
    "        print('每两个请求重新获取cookie，重置完成后，现在是第%s个'%str(self.count))\n",
    "        if(self.count == 2):\n",
    "            self.cookies = self.get_cookies()\n",
    "            self.count = 0\n",
    "        html = requests.get(url, headers=self.headers,\n",
    "                            cookies=self.cookies).content\n",
    "#         print(html)\n",
    "        html = etree.HTML(html)\n",
    "#         print('第%s页'%str(int(i/20+1)))\n",
    "#         print(html)\n",
    "        return html\n",
    "\n",
    "    # 对第i页网页列表逐个进行解析获取细节数据\n",
    "    def get_list_url(self, html):\n",
    "        # xpath具备较多缺陷，当在谷歌xpath插件上构建成功后，可以考虑删除tbody作为当前脚本的构建结果\n",
    "        list_url = html.xpath('//*[@id=\"chartForm\"]/div/div[2]/table//tr/td[2]/a/@href')  \n",
    "#         print(list_url)\n",
    "        for url0 in list_url:\n",
    "            url = self.__link__ + url0\n",
    "            self.parse_detail(url)\n",
    "\n",
    "    # 解析缓存网页的详细信息\n",
    "    def parse_loadedhtml(self, html,url):\n",
    "        # item = OrderedDict()  # 如果要存入csv文档，建议用有序字典\n",
    "        item = OrderedDict()\n",
    "        # 获取cnvd id\n",
    "        list_url=html.xpath('/html/head/title/text()')[0]\n",
    "        # list_url.split()[0]\n",
    "        if 'CVE' in list_url.split()[0]:\n",
    "            item[\"app_name\"] = 'Palo Alto Networks'\n",
    "            item[\"cve_list\"] = list_url.split()[0]\n",
    "            \n",
    "            # 获取漏洞影响的产品版本号\n",
    "            list_version = html.xpath(\n",
    "                '//table[@class=\"neat card\"]//td/text()')\n",
    "            # list_version\n",
    "            # ''.join(list_version)\n",
    "            versions={}\n",
    "            if 'Windows' not in ''.join(list_version) and 'OS X' not in ''.join(list_version):\n",
    "                for i in range(len(list_version)):\n",
    "                    if '<' in list_version[i]:\n",
    "                        versions[list_version[i-1]]=''.join(list(filter(lambda ch: ch in '0123456789.', list_version[i])))\n",
    "                item[\"packages\"] = versions\n",
    "#                 print(item[\"packages\"])\n",
    "            \n",
    "                # 获取sa_id\n",
    "                item[\"sa_id\"] = html.xpath(\n",
    "                    \"//h4/text()\")\n",
    "                if 'PAN-OS' in item[\"sa_id\"][0]:\n",
    "                    item[\"sa_id\"] = \"_\".join((item[\"cve_list\"],item[\"sa_id\"][0]))\n",
    "\n",
    "                    # 获得sa_link\n",
    "                    item[\"sa_link\"] = url\n",
    "\n",
    "                    # 获得sa_title\n",
    "                    item[\"sa_title\"] = html.xpath('/html/head/title/text()')\n",
    "                    item[\"sa_title\"] = item[\"sa_title\"][0]\n",
    "\n",
    "                    # 获得sa_url\n",
    "                    item[\"sa_url\"] = url\n",
    "                    \n",
    "                    # 保存为json格式\n",
    "                    item = [item]\n",
    "                    # 输出：[OrderedDict([('target', 'total_result'), ('key1', 'value1'), ('key2', 'value2'), ('key3', 'value3')])]\n",
    "                    current_dir = os.path.dirname(self.__file__)\n",
    "                    path = os.path.join(os.path.join(current_dir), self.__file__.split('/')[-1])\n",
    "#                     print(3)\n",
    "#                     print(path)\n",
    "#                     input()\n",
    "                    with open(path, 'a') as f:\n",
    "                        json.dump(item, f, encoding=\"utf-8\", ensure_ascii=False, indent=4, separators=(',', ':'))\n",
    "    \n",
    "    # 获取详细漏洞信息数据，采用get请求方法\n",
    "    def parse_detail(self, url):\n",
    "#         time.sleep(random.randint(5, 10))\n",
    "        html = self.parsedetail(url)\n",
    "        # item = OrderedDict()  # 如果要存入csv文档，建议用有序字典\n",
    "        item = OrderedDict()\n",
    "        \n",
    "        # 获取cnvd id\n",
    "        list_url=html.xpath('/html/head/title/text()')[0]\n",
    "        # list_url.split()[0]\n",
    "        if 'CVE' in list_url.split()[0]:\n",
    "            item[\"app_name\"] = 'Palo Alto Networks'\n",
    "            item[\"cve_list\"] = list_url.split()[0]\n",
    "            \n",
    "            # 获取漏洞影响的产品版本号\n",
    "            list_version = html.xpath(\n",
    "                '//table[@class=\"neat card\"]//td/text()')\n",
    "            # list_version\n",
    "            # ''.join(list_version)\n",
    "            versions={}\n",
    "            if 'Windows' not in ''.join(list_version) and 'OS X' not in ''.join(list_version):\n",
    "                for i in range(len(list_version)):\n",
    "                    if '<' in list_version[i]:\n",
    "                        versions[list_version[i-1]]=''.join(list(filter(lambda ch: ch in '0123456789.', list_version[i])))\n",
    "                item[\"packages\"] = versions\n",
    "#                 print(item[\"packages\"])\n",
    "            \n",
    "                # 获取sa_id\n",
    "                item[\"sa_id\"] = html.xpath(\n",
    "                    \"//h4/text()\")\n",
    "                if 'PAN-OS' in item[\"sa_id\"][0]:\n",
    "                    item[\"sa_id\"] = \"_\".join((item[\"cve_list\"],item[\"sa_id\"][0]))\n",
    "\n",
    "                    # 获得sa_link\n",
    "                    item[\"sa_link\"] = url\n",
    "\n",
    "                    # 获得sa_title\n",
    "                    item[\"sa_title\"] = html.xpath('/html/head/title/text()')\n",
    "                    item[\"sa_title\"] = item[\"sa_title\"][0]\n",
    "\n",
    "                    # 获得sa_url\n",
    "                    item[\"sa_url\"] = url\n",
    "                    \n",
    "                    # 保存为json格式\n",
    "                    item = [item]\n",
    "                    # 输出：[OrderedDict([('target', 'total_result'), ('key1', 'value1'), ('key2', 'value2'), ('key3', 'value3')])]\n",
    "                    current_dir = os.path.dirname(self.__file__)\n",
    "                    path = os.path.join(os.path.join(current_dir), self.__file__.split('/')[-1])\n",
    "                                        \n",
    "                    print(url)\n",
    "                    local_filenames = os.path.join(os.path.dirname(self.__file__), self.__loadeddir__)\n",
    "                    local_filenames = local_filenames+'/'+url.split('/')[-1]+'.html'\n",
    "                    print(local_filenames) #C:/Users/wz/Desktop/lvmeng\\loadedhtml\n",
    "#                     input()\n",
    "                    self.download_document(url, local_filenames,1)\n",
    "                    \n",
    "                    with open(path, 'a') as f:\n",
    "                        json.dump(item, f, encoding=\"utf-8\", ensure_ascii=False, indent=4, separators=(',', ':'))\n",
    "\n",
    "    def run(self):        \n",
    "        chrome_options = Options()\n",
    "        # 加上下面两行，解决报错\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "#         chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.binary_location =  r\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"\n",
    "        driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "        #设置每次ChromeDriver访问的初始页面用来更新cookie，以绕过cnvd爬虫限制 OK\n",
    "        driver.get(\"https://openapi.book118.com/login/login/index.html\")\n",
    "        raw_input()\n",
    "        # 进入“我的文档”\n",
    "        driver.find_element_by_xpath(\"//ul[@id='nav']/li[4]/a/span\").click()\n",
    "        time.sleep(3)\n",
    "        # 进入“回收站”\n",
    "        driver.find_element_by_xpath(\"//div[@id='broadside']/div/ul/li[8]/a/span/em\").click()\n",
    "        time.sleep(3)\n",
    "        # 切换到frame框架\n",
    "        driver.switch_to.frame(0)\n",
    "        time.sleep(3)\n",
    "        driver.find_element_by_xpath(\"//div[@id='batch']/div/div/label\").click()\n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_link_text(u\"批量彻底删除\").click()\n",
    "        time.sleep(3)\n",
    "        # 切换到默认窗口确认删除\n",
    "        driver.switch_to_default_content()\n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_xpath(\"//div[@class='layui-layer-btn layui-layer-btn-c']/a[1]\").click()\n",
    "        time.sleep(7)\n",
    "        for i in range(1,300,1):\n",
    "            time.sleep(3)\n",
    "            # 切换到frame框架\n",
    "            driver.switch_to.frame(0)\n",
    "            time.sleep(3)\n",
    "            driver.find_element_by_xpath(\"//div[@id='batch']/div/div/label\").click()\n",
    "            time.sleep(2)\n",
    "            driver.find_element_by_link_text(u\"批量彻底删除\").click()\n",
    "            time.sleep(3)\n",
    "            # 切换到默认窗口确认删除\n",
    "            driver.switch_to_default_content()\n",
    "            time.sleep(2)\n",
    "            driver.find_element_by_xpath(\"//div[@class='layui-layer-btn layui-layer-btn-c']/a[1]\").click()\n",
    "            time.sleep(7)\n",
    "        raw_input()\n",
    "        if is_crwal == 1:\n",
    "            for i in range(1,10,1): \n",
    "                print('第%s页'%str(i))\n",
    "                html = self.parse(i)\n",
    "    #             print(i) #页数\n",
    "                # 解析细节\n",
    "                next_url = self.get_list_url(html)\n",
    "        # 直接从本地缓存网页中捕获目标信息\n",
    "        else:\n",
    "            g = os.walk(os.path.join(os.path.dirname(self.__file__),self.__loadeddir__))\n",
    "            for path,dir_list,file_list in g:  \n",
    "                for file_name in file_list:  \n",
    "                    file_name = os.path.join(path, file_name)\n",
    "                    fp = open(file_name,'rb')\n",
    "                    html_new = fp.read()\n",
    "                    url = self.__link__+'/'+file_name.split('\\\\')[-1]\n",
    "                    print(1)\n",
    "                    print(url)\n",
    "#                     input()\n",
    "#                     print(html_new)\n",
    "#                     import re\n",
    "                    content=etree.HTML(html_new)\n",
    "                    self.parse_loadedhtml(content,url)\n",
    "#                     print(content)\n",
    "            \n",
    "        # 对所有数据进行json化\n",
    "        f = open(self.__file__,'a+')\n",
    "        s = f.read()\n",
    "        s = s.replace('][',',')\n",
    "        f.seek(0)\n",
    "        f.truncate() #把原来的内容删掉\n",
    "        f.write(s)\n",
    "        f.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a = Wenkuspider()\n",
    "    pool = Threadpool(1)  # 单线程跑的慢，但是基本很稳定没有被cnvd封杀。本次跑完整个cnvd库大概十天左右，期间网络波动中断需要维护代码，及爬虫重新爬的页面范围。\n",
    "    a.run()\n",
    "    # pool.map(a.run(),self.parse())\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "time.sleep(5)\n",
    "print(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
